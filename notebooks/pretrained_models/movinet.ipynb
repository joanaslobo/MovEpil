{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Movinet\n",
    "\n",
    "This notebook is a concept test for the Movinet action recognition model from Google. This model allows to classify movement sequences in stream, is light as is already compiled and optimized for inference in TF lite."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b6b1499572fd8f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import random\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:06:48.335807Z",
     "start_time": "2024-05-15T08:06:46.611938Z"
    }
   },
   "id": "a22ddb234733baa4",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1 - Load the labels from Kinetics 600"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b2fbabf5f1d500c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:06:48.342094Z",
     "start_time": "2024-05-15T08:06:48.337031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['abseiling', 'acting in play', 'adjusting glasses', 'air drumming',\n       'alligator wrestling', 'answering questions', 'applauding',\n       'applying cream', 'archaeological excavation', 'archery',\n       'arguing', 'arm wrestling', 'arranging flowers',\n       'assembling bicycle', 'assembling computer',\n       'attending conference', 'auctioning', 'backflip (human)',\n       'baking cookies', 'bandaging'], dtype='<U49')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_path = tf.keras.utils.get_file(\n",
    "    fname='labels.txt',\n",
    "    origin='https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt'\n",
    ")\n",
    "\n",
    "labels_path = pathlib.Path(labels_path)\n",
    "\n",
    "lines = labels_path.read_text().splitlines()\n",
    "KINETICS_600_LABELS = np.array([line.strip() for line in lines])\n",
    "KINETICS_600_LABELS[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Load a pretrained model from tensorflow hub.\n",
    "\n",
    "This is now integrated with Kaggle, there are several options to choose here. From base version to stream version that can be used live.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20a5de66270739b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "id = 'a2'\n",
    "mode = 'base'\n",
    "version = '3'\n",
    "hub_url = f'https://tfhub.dev/tensorflow/movinet/{id}/{mode}/kinetics-600/classification/{version}'\n",
    "model = hub.load(hub_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:06:54.510530Z",
     "start_time": "2024-05-15T08:06:48.342843Z"
    }
   },
   "id": "e52c67662d0a741e",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check model signatures\n",
    "\n",
    "Where the data is going to be loaded and the call to make inferences over that data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ce9bf6a35ed2cf4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['serving_default']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.signatures.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:06:54.514299Z",
     "start_time": "2024-05-15T08:06:54.511845Z"
    }
   },
   "id": "720271201e2ea2a9",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils function\n",
    "\n",
    "To read and format each frame to be consumed by the model, we need first extract each frame and format it, and for formatting the predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b85ff1bbf82b784"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame\n",
    "\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "# Get top_k labels and probabilities\n",
    "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
    "  \"\"\"Outputs the top k model labels and probabilities on the given video.\n",
    "\n",
    "  Args:\n",
    "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
    "      the probability of each class on each frame.\n",
    "    k: the number of top predictions to select.\n",
    "    label_map: a list of labels to map logit indices to label strings.\n",
    "\n",
    "  Returns:\n",
    "    a tuple of the top-k labels and probabilities.\n",
    "  \"\"\"\n",
    "  # Sort predictions to find top_k\n",
    "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
    "  # collect the labels of top_k predictions\n",
    "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
    "  # decode lablels\n",
    "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
    "  # top_k probabilities of the predictions\n",
    "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
    "  return tuple(zip(top_labels, top_probs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:06:54.520389Z",
     "start_time": "2024-05-15T08:06:54.515170Z"
    }
   },
   "id": "6cc285753b93d3b0",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 - Load sample video from KTH dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a32c845b09df890"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(25, 224, 224, 3)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxing = frames_from_video_file(video_path=\"boxing_sample.avi\", n_frames=25, output_size=(224,224), frame_step=15) #load_gif(jumpingjack_path)\n",
    "boxing.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:06:54.584107Z",
     "start_time": "2024-05-15T08:06:54.521161Z"
    }
   },
   "id": "1a1e94485373c7fe",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 - Get the inference method reference and make predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa00474e1f3a9ef2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715760415.137757 6920634 service.cc:145] XLA service 0x6000021a8000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1715760415.137788 6920634 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1715760415.143451 6920634 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "sig = model.signatures['serving_default']\n",
    "\n",
    "logits = sig(image = boxing[tf.newaxis, ...])\n",
    "logits = logits['classifier_head'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:07:01.896707Z",
     "start_time": "2024-05-15T08:06:54.584814Z"
    }
   },
   "id": "19fd6e0afac86318",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact juggling    : 0.345\n",
      "robot dancing       : 0.069\n",
      "finger snapping     : 0.038\n",
      "busking             : 0.030\n",
      "flying kite         : 0.029\n"
     ]
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits, axis=-1)\n",
    "for label, p in get_top_k(probs):\n",
    "  print(f'{label:20s}: {p:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T08:07:01.903124Z",
     "start_time": "2024-05-15T08:07:01.897465Z"
    }
   },
   "id": "a9e60566e4442cf8",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
